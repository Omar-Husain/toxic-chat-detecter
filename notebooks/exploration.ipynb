{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üéÆ Toxic Chat Detection - Data Exploration\n",
        "\n",
        "This notebook explores the sample dataset for our toxic chat detection project.\n",
        "\n",
        "**Objectives:**\n",
        "- Load and inspect the data\n",
        "- Analyze class distribution\n",
        "- Explore text characteristics\n",
        "- Visualize word frequencies\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standard imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "\n",
        "# Configure display settings\n",
        "pd.set_option('display.max_colwidth', 100)\n",
        "\n",
        "print(\"Libraries loaded successfully! ‚úÖ\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load the Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the sample dataset\n",
        "df = pd.read_csv('../data/sample_data.csv')\n",
        "\n",
        "# Basic info\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"Columns: {list(df.columns)}\")\n",
        "print(f\"\\nData types:\")\n",
        "print(df.dtypes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display first few rows\n",
        "print(\"\\nüìù Sample rows:\")\n",
        "df.head(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Class Distribution Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Count of each class\n",
        "class_counts = df['label'].value_counts()\n",
        "print(\"\\nüìä Class Distribution:\")\n",
        "print(f\"  Non-Toxic (0): {class_counts[0]} ({class_counts[0]/len(df)*100:.1f}%)\")\n",
        "print(f\"  Toxic (1):     {class_counts[1]} ({class_counts[1]/len(df)*100:.1f}%)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize class distribution\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "# Bar chart\n",
        "colors = ['#2ecc71', '#e74c3c']\n",
        "axes[0].bar(['Non-Toxic', 'Toxic'], class_counts.values, color=colors)\n",
        "axes[0].set_title('Class Distribution (Count)', fontsize=12)\n",
        "axes[0].set_ylabel('Number of Messages')\n",
        "for i, v in enumerate(class_counts.values):\n",
        "    axes[0].text(i, v + 0.5, str(v), ha='center', fontsize=12, fontweight='bold')\n",
        "\n",
        "# Pie chart\n",
        "axes[1].pie(class_counts.values, labels=['Non-Toxic', 'Toxic'], \n",
        "            autopct='%1.1f%%', colors=colors, startangle=90,\n",
        "            explode=(0, 0.05))\n",
        "axes[1].set_title('Class Distribution (Percentage)', fontsize=12)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Check balance\n",
        "balance_ratio = min(class_counts) / max(class_counts)\n",
        "print(f\"\\n‚öñÔ∏è Class balance ratio: {balance_ratio:.2f}\")\n",
        "if balance_ratio > 0.8:\n",
        "    print(\"   ‚Üí Dataset is well-balanced!\")\n",
        "elif balance_ratio > 0.5:\n",
        "    print(\"   ‚Üí Dataset has moderate imbalance\")\n",
        "else:\n",
        "    print(\"   ‚Üí Dataset is imbalanced - consider class weights or oversampling\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Text Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate text statistics\n",
        "df['word_count'] = df['text'].apply(lambda x: len(str(x).split()))\n",
        "df['char_count'] = df['text'].apply(lambda x: len(str(x)))\n",
        "\n",
        "print(\"üìè Text Length Statistics:\")\n",
        "print(f\"\\n  Word count:\")\n",
        "print(f\"    Min: {df['word_count'].min()}\")\n",
        "print(f\"    Max: {df['word_count'].max()}\")\n",
        "print(f\"    Mean: {df['word_count'].mean():.1f}\")\n",
        "print(f\"    Median: {df['word_count'].median():.1f}\")\n",
        "\n",
        "print(f\"\\n  Character count:\")\n",
        "print(f\"    Min: {df['char_count'].min()}\")\n",
        "print(f\"    Max: {df['char_count'].max()}\")\n",
        "print(f\"    Mean: {df['char_count'].mean():.1f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Word Frequency Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_word_freq(texts):\n",
        "    \"\"\"Get word frequencies from a list of texts.\"\"\"\n",
        "    all_words = []\n",
        "    for text in texts:\n",
        "        words = str(text).lower().split()\n",
        "        all_words.extend(words)\n",
        "    return Counter(all_words)\n",
        "\n",
        "# Get word frequencies by class\n",
        "non_toxic_texts = df[df['label'] == 0]['text'].tolist()\n",
        "toxic_texts = df[df['label'] == 1]['text'].tolist()\n",
        "\n",
        "non_toxic_words = get_word_freq(non_toxic_texts)\n",
        "toxic_words = get_word_freq(toxic_texts)\n",
        "\n",
        "print(\"üî§ Top 10 words in NON-TOXIC messages:\")\n",
        "for word, count in non_toxic_words.most_common(10):\n",
        "    print(f\"   {word}: {count}\")\n",
        "\n",
        "print(\"\\n‚ö†Ô∏è Top 10 words in TOXIC messages:\")\n",
        "for word, count in toxic_words.most_common(10):\n",
        "    print(f\"   {word}: {count}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize top words\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Non-toxic words\n",
        "top_non_toxic = non_toxic_words.most_common(10)\n",
        "words, counts = zip(*top_non_toxic)\n",
        "axes[0].barh(words, counts, color='#2ecc71')\n",
        "axes[0].set_xlabel('Frequency')\n",
        "axes[0].set_title('Top 10 Words in Non-Toxic Messages')\n",
        "axes[0].invert_yaxis()\n",
        "\n",
        "# Toxic words\n",
        "top_toxic = toxic_words.most_common(10)\n",
        "words, counts = zip(*top_toxic)\n",
        "axes[1].barh(words, counts, color='#e74c3c')\n",
        "axes[1].set_xlabel('Frequency')\n",
        "axes[1].set_title('Top 10 Words in Toxic Messages')\n",
        "axes[1].invert_yaxis()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Example Messages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show example messages from each class\n",
        "print(\"‚úÖ Example NON-TOXIC messages:\")\n",
        "print(\"-\" * 50)\n",
        "for i, text in enumerate(df[df['label'] == 0]['text'].head(5), 1):\n",
        "    print(f'{i}. \"{text}\"')\n",
        "\n",
        "print(\"\\n‚ö†Ô∏è Example TOXIC messages:\")\n",
        "print(\"-\" * 50)\n",
        "for i, text in enumerate(df[df['label'] == 1]['text'].head(5), 1):\n",
        "    print(f'{i}. \"{text}\"')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Summary & Next Steps\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*60)\n",
        "print(\"üìã DATA EXPLORATION SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nüìä Dataset Statistics:\")\n",
        "print(f\"   Total samples: {len(df)}\")\n",
        "print(f\"   Non-toxic: {class_counts[0]} ({class_counts[0]/len(df)*100:.1f}%)\")\n",
        "print(f\"   Toxic: {class_counts[1]} ({class_counts[1]/len(df)*100:.1f}%)\")\n",
        "print(f\"   Average word count: {df['word_count'].mean():.1f}\")\n",
        "print(f\"\\nüîë Key Observations:\")\n",
        "print(f\"   - Dataset is well-balanced (50/50 split)\")\n",
        "print(f\"   - Short messages typical of chat (avg ~5 words)\")\n",
        "print(f\"   - Toxic messages contain negative words\")\n",
        "print(f\"   - Non-toxic messages are encouraging/positive\")\n",
        "print(f\"\\nüìù Next Steps:\")\n",
        "print(f\"   1. Train baseline model: python src/train_baseline.py\")\n",
        "print(f\"   2. Train PyTorch model: python src/train_torch.py\")\n",
        "print(f\"   3. Run web app: streamlit run app/app_streamlit.py\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\n‚úÖ Exploration complete! Ready for model training.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
